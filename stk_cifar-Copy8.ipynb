{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform (-1, 1) Cl par. 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from utils import arg_parser_handler,  show, distribution_select\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No args!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--n_epochs N_EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--lr LR] [--b1 B1]\n",
      "                             [--b2 B2] [--n_cpu N_CPU]\n",
      "                             [--latent_dim LATENT_DIM] [--img_size IMG_SIZE]\n",
      "                             [--channels CHANNELS]\n",
      "                             [--sample_interval SAMPLE_INTERVAL]\n",
      "                             [--n_paths_G N_PATHS_G]\n",
      "                             [--classifier_para CLASSIFIER_PARA]\n",
      "                             [--vae_name VAE_NAME] [--dim1 DIM1] [--dim2 DIM2]\n",
      "                             [--min_size_dataset MIN_SIZE_DATASET]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/corsair/.local/share/jupyter/runtime/kernel-624b5659-917b-42bd-a394-8ccae60cb38b.json\n"
     ]
    }
   ],
   "source": [
    "#Inicializa inputs por meio de notebook (variaveis de classe) ou por terminal (argparse)\n",
    "args, flag = arg_parser_handler().parser_maker()\n",
    "if flag == 1:\n",
    "    args.batch_size=64\n",
    "    args.n_epochs=10\n",
    "    args.vae_name = None\n",
    "    args.dim1 = 256\n",
    "    args.dim2 = 64\n",
    "    args.n_paths_G = 10\n",
    "    args.classifier_para=0.1\n",
    "    print('No args!')\n",
    "elif flag == 0:\n",
    "    print(args)\n",
    "\n",
    "CUDA = True if torch.cuda.is_available() else False\n",
    "VAE_FOLDER = 'vae_pretrained/'\n",
    "IMG_SHAPE = (3, 32, 32)\n",
    "SHARED_CLASSIFIER = True\n",
    "DISTRIBUTION = 'uniform'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Configure data loader\n",
    "os.makedirs('../data/cifar', exist_ok=True)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data/cifar', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize([.5],[.5],[.5])\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if CUDA else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_paths_G = 2, latent_dim = 100, nz=100, nc = 3, ngf=128):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.n_paths_G = n_paths_G\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        modules = nn.ModuleList()\n",
    "        for _ in range(self.n_paths_G):\n",
    "            modules.append(nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.Linear(latent_dim, ngf * 2 * 8 * 8),\n",
    "            nn.ReLU(True),\n",
    "            Reshape(-1, ngf * 2, 8, 8),\n",
    "            # state size. (ngf*2) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 16 x 16\n",
    "            nn.ConvTranspose2d(    ngf,      nc, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "            # output size. (3) x 32 x 32\n",
    "        ))\n",
    "            \n",
    "        self.paths = modules\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = []\n",
    "        \n",
    "        for path in self.paths:\n",
    "            img.append(path(z))\n",
    "        img = torch.cat(img, dim=0)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 32, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, _ = next(iter(dataloader))\n",
    "a.shape\n",
    "gen = Generator().cuda()\n",
    "z = Variable(Tensor(np.random.normal(0, 1, (args.batch_size, args.latent_dim))))\n",
    "out_gen = gen.paths[0](z)\n",
    "out_gen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc=3, ndf=128, n_paths_G = 2, init_sample = None):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.n_paths_G = n_paths_G\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            # input is (nc) x 32 x 32\n",
    "            nn.Conv2d(nc, ndf, kernel_size=5, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf, momentum = 0.9),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 16 x 16\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=5, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2, momentum = 0.9),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "        )\n",
    "        modules = nn.ModuleList()\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "\n",
    "                # state size. (ndf*2) x 8 x 8\n",
    "                nn.Conv2d(ndf * 2, ndf * 4, kernel_size=5, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 4, momentum = 0.9),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "                \n",
    "                # state size. (ndf*4) x 4 x 4 \n",
    "            ))\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "\n",
    "                # state size. (ndf*2) x 8 x 8\n",
    "                nn.Conv2d(ndf * 2, ndf * 4, kernel_size=5, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 4, momentum = 0.9),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "               \n",
    "                # state size. (ndf*4) x 4 x 4\n",
    "            ))\n",
    "        \n",
    "        self.paths = modules\n",
    "       \n",
    "        for i, ly in enumerate(self.shared):\n",
    "            #print(type(ly) == torch.nn.modules.conv.Conv2d)\n",
    "            if (type(ly) == torch.nn.modules.conv.Conv2d):\n",
    "                shape = self.shared[:i+1](init_sample).shape\n",
    "                print('layer: {}'.format(i))\n",
    "                print(ly)\n",
    "                print('output shape: {}'.format(shape))\n",
    "                \n",
    "        for i, ly in enumerate(self.paths[0]):\n",
    "            #print(type(ly) == torch.nn.modules.conv.Conv2d)\n",
    "            if (type(ly) == torch.nn.modules.conv.Conv2d):\n",
    "                shape =  self.paths[0][:i+1](self.shared(init_sample)).shape\n",
    "                print('layer: {}'.format(i))\n",
    "                print(ly)\n",
    "                print('output shape: {}'.format(shape))\n",
    "                \n",
    "        total_units = self.paths[0](self.shared(init_sample)).view(args.batch_size, -1).shape[-1]\n",
    "        print(\"Total Units : {}\".format(total_units))\n",
    "        \n",
    "        self.linear_disc  = nn.Linear(total_units, 1)\n",
    "        self.linear_class = nn.Linear(total_units, n_paths_G)\n",
    "        \n",
    "    def forward(self, img):\n",
    "            \n",
    "        shared_output = self.shared(img)\n",
    "        validity = (self.paths[0](shared_output)).view(img.shape[0], -1)\n",
    "        validity = self.sigmoid(self.linear_disc(validity))\n",
    "        \n",
    "        classification = (self.paths[1](shared_output)).view(img.shape[0], -1)\n",
    "        classification = self.linear_class(classification)\n",
    "        classification = self.log_softmax(classification)\n",
    "        \n",
    "        #validity = (self.paths[0](shared_output)).view(-1, 1)\n",
    "        #classifier = (self.paths[1](shared_output)).view(-1, self.n_paths_G).squeeze(1)\n",
    "        \n",
    "        return validity, classification\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0\n",
      "Conv2d(3, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "output shape: torch.Size([64, 128, 15, 15])\n",
      "layer: 3\n",
      "Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "output shape: torch.Size([64, 256, 7, 7])\n",
      "layer: 0\n",
      "Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "output shape: torch.Size([64, 512, 3, 3])\n",
      "Total Units : 4608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_gen.shape\n",
    "\n",
    "init_tensor = torch.zeros(args.batch_size,3,32,32)\n",
    "\n",
    "disc = Discriminator(init_sample = init_tensor, n_paths_G=args.n_paths_G).cuda()\n",
    "out_disc = disc(out_gen)\n",
    "out_disc[0].shape\n",
    "\n",
    "out_disc[1].shape\n",
    "\n",
    "args.n_paths_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"out_disc[0].shape\\n\\nvalid = Variable(Tensor(100, 1).fill_(1.0), requires_grad=False)\\n\\nvalid.shape\\n\\nz = Variable(Tensor(np.random.normal(0, 1, (100, 100))))\\n\\ngen_imgs = gen.paths[0](z)\\n\\nvalidity, classification = disc(gen_imgs)\\n# Loss measures generator's ability to fool the discriminator\\n\\n\\n\\ntarget = Variable(Tensor(100).fill_(1), requires_grad=False)\\ntarget = target.type(torch.cuda.LongTensor)\\n\\n\\n\\nF.nll_loss(classification, target)\\n\\n\\nadversarial_loss(validity, valid)\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''out_disc[0].shape\n",
    "\n",
    "valid = Variable(Tensor(100, 1).fill_(1.0), requires_grad=False)\n",
    "\n",
    "valid.shape\n",
    "\n",
    "z = Variable(Tensor(np.random.normal(0, 1, (100, 100))))\n",
    "\n",
    "gen_imgs = gen.paths[0](z)\n",
    "\n",
    "validity, classification = disc(gen_imgs)\n",
    "# Loss measures generator's ability to fool the discriminator\n",
    "\n",
    "\n",
    "\n",
    "target = Variable(Tensor(100).fill_(1), requires_grad=False)\n",
    "target = target.type(torch.cuda.LongTensor)\n",
    "\n",
    "\n",
    "\n",
    "F.nll_loss(classification, target)\n",
    "\n",
    "\n",
    "adversarial_loss(validity, valid)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generators_confusion_matrix(generator, discriminator, samples = 100):\n",
    "\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    acc_all_gens = np.zeros((args.n_paths_G, args.n_paths_G))\n",
    "    temp = []\n",
    "\n",
    "    for s in range(samples):\n",
    "        z = Variable(Tensor(distribution_select(DISTRIBUTION, (args.batch_size, args.latent_dim))))\n",
    "        for k in range(args.n_paths_G):\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator.paths[k](z)\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "\n",
    "            validity, classification = discriminator(gen_imgs)\n",
    "            # Loss measures classifier's ability }to classify various generators\n",
    "\n",
    "            target = Variable(Tensor(args.batch_size).fill_(k), requires_grad=False)\n",
    "            target = target.type(torch.cuda.LongTensor)\n",
    "\n",
    "            acc_gen_k = []\n",
    "\n",
    "            for target in range(args.n_paths_G):\n",
    "                acc = ((classification.argmax(dim=1))==target).sum().cpu().numpy()/args.batch_size\n",
    "                acc_gen_k.append(acc)\n",
    "\n",
    "            acc_all_gens[k] = (np.array(acc_all_gens[k]) + np.array(acc_gen_k))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return acc_all_gens/samples\n",
    "\n",
    "def plot_confusion_matrix(matrix):\n",
    "    df_cm = pd.DataFrame(matrix)\n",
    "    plt.figure(figsize = (10,8))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    plt.xlabel('RESPOSTA DO CLASSIFICADOR')\n",
    "    plt.ylabel('IMAGENS DE CADA GERADOR')\n",
    "    plt.show() # ta-da!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(dataloader, generator, discriminator,\n",
    "                 adversarial_loss, optimizer_D, optimizer_G, \n",
    "                 classifier = None, optimizer_C = None, show_logs = False):\n",
    "       \n",
    "    for epoch in tqdm_notebook(range(args.n_epochs)):\n",
    "\n",
    "        start = time.time()\n",
    "        epoch_start = 0\n",
    "        g_loss_epoch = 0\n",
    "        d_loss_epoch = 0\n",
    "        c_loss_1_epoch = 0\n",
    "        c_loss_2_epoch = 0\n",
    "        num_batches = len(dataloader)\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "            # Configure input\n",
    "            real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Sample noise as generator input\n",
    "            z = Variable(Tensor(distribution_select(DISTRIBUTION, (args.batch_size, args.latent_dim))))\n",
    "\n",
    "            g_loss = 0\n",
    "            c_loss_1 = 0\n",
    "            gen_sequence1 = np.arange(args.n_paths_G)\n",
    "            np.random.shuffle(gen_sequence1)\n",
    "            for k in gen_sequence1:\n",
    "\n",
    "                # Generate a batch of images\n",
    "                gen_imgs = generator.paths[k](z)\n",
    "\n",
    "                # Loss measures generator's ability to fool the discriminator\n",
    "                \n",
    "                validity, classification = discriminator(gen_imgs)\n",
    "                g_loss += adversarial_loss(validity, valid)\n",
    "\n",
    "                # Loss measures classifier's ability to classify various generators\n",
    "                \n",
    "                target = Variable(Tensor(imgs.size(0)).fill_(k), requires_grad=False)\n",
    "                target = target.type(torch.cuda.LongTensor)\n",
    "                \n",
    "                if SHARED_CLASSIFIER == False:\n",
    "                    classification = classifier(gen_imgs)\n",
    "                \n",
    "                c_loss_1 += F.nll_loss(classification, target)*args.classifier_para\n",
    "\n",
    "            g_loss_epoch += g_loss\n",
    "            c_loss_1_epoch += c_loss_1\n",
    "\n",
    "            g_loss = g_loss + c_loss_1\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ------------------------------------\n",
    "            #  Train Discriminator and Classifier\n",
    "            # ------------------------------------\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            if (SHARED_CLASSIFIER == False): \n",
    "                optimizer_C.zero_grad()\n",
    "            \n",
    "            d_loss = 0\n",
    "            c_loss_2 = 0\n",
    "            \n",
    "            \n",
    "            validity, classification = discriminator(real_imgs)\n",
    "            real_loss = adversarial_loss(validity, valid)\n",
    "            d_loss += real_loss\n",
    "            temp = []\n",
    "            \n",
    "            gen_sequence2 = np.arange(args.n_paths_G)\n",
    "            np.random.shuffle(gen_sequence2)\n",
    "            for k in gen_sequence2:\n",
    "\n",
    "                # Generate a batch of images\n",
    "                gen_imgs = generator.paths[k](z).view(imgs.shape[0], *IMG_SHAPE)\n",
    "                temp.append(gen_imgs[0:20, :])\n",
    "                  \n",
    "                \n",
    "                # Loss measures discriminator's ability to classify real from generated samples\n",
    "                validity, classification = discriminator(gen_imgs.detach())\n",
    "                fake_loss = adversarial_loss(validity, fake)\n",
    "                d_loss += fake_loss\n",
    "\n",
    "                # Loss measures classifier's ability to classify various generators\n",
    "                target = Variable(Tensor(imgs.size(0)).fill_(k), requires_grad=False)\n",
    "                target = target.type(torch.cuda.LongTensor)\n",
    "                \n",
    "                if (SHARED_CLASSIFIER == False):\n",
    "                    classification = classifier(gen_imgs)\n",
    "                c_loss_2 += F.nll_loss(classification, target)*args.classifier_para\n",
    "\n",
    "            plot_imgs = torch.cat(temp, dim=0)\n",
    "\n",
    "            d_loss_epoch += d_loss\n",
    "            c_loss_2_epoch += c_loss_2\n",
    "            \n",
    "\n",
    "            d_loss = d_loss + c_loss_2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "                \n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "\n",
    "        interval = time.time() - start\n",
    "\n",
    "        if epoch % (args.n_epochs//5) == 0 and show_logs:\n",
    "\n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [C loss 1: %f] [C loss 2: %f] \\t Time Interv: %f\"\n",
    "                   % (epoch, args.n_epochs, i, num_batches, d_loss_epoch.item()/num_batches, g_loss_epoch.item()/num_batches, \n",
    "                      c_loss_1_epoch.item()/num_batches, c_loss_2_epoch.item()/num_batches, interval))\n",
    "            print(gen_sequence1)\n",
    "            print(gen_sequence2)\n",
    "            show(make_grid(plot_imgs.cpu(), nrow=20, normalize=True), 20)\n",
    "            plot_confusion_matrix(generators_confusion_matrix(generator, discriminator))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tensor = torch.zeros(args.batch_size,3,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0\n",
      "Conv2d(3, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "output shape: torch.Size([64, 128, 15, 15])\n",
      "layer: 3\n",
      "Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "output shape: torch.Size([64, 256, 7, 7])\n",
      "layer: 0\n",
      "Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "output shape: torch.Size([64, 512, 3, 3])\n",
      "Total Units : 4608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corsair/.conda/envs/torch_gan/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c8e25b1e5a41a888e358154c160adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "branch_counter = 0\n",
    "gen_stack= []\n",
    "disc_stack =[]\n",
    "data_loader_stack = []\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "generator = Generator(n_paths_G=args.n_paths_G)  \n",
    "discriminator = Discriminator(n_paths_G=args.n_paths_G, init_sample=init_tensor)\n",
    "        \n",
    "if CUDA:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=args.lr, betas=(args.b1, args.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=args.lr, betas=(args.b1, args.b2))\n",
    "\n",
    "train_models(dataloader, generator, discriminator, adversarial_loss, \n",
    "             optimizer_D, optimizer_G, show_logs=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_models(dataloader, generator, discriminator, adversarial_loss, \n",
    "             optimizer_D, optimizer_G, show_logs=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_models(dataloader, generator, discriminator, adversarial_loss, \n",
    "             optimizer_D, optimizer_G, show_logs=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_models(dataloader, generator, discriminator, adversarial_loss, \n",
    "             optimizer_D, optimizer_G, show_logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
